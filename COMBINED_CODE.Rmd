---
title: "Bank Marketing_Group 1"
author: "Antariksh Mishra b00495417  Joël Rozencweig b00399577  Xiyan QIN b00690235  Zhixian Cui b00562229"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
install.packages("Amelia")
install.packages("mice")
install.packages("VIM")
install.packages("pscl")
install.packages("gplots")
install.packages("pROC")
install.packages("ROCR")
install.packages("FactoMineR")
install.packages("randomForest")
install.packages("psych")
install.packages("ggplot2")
install.packages("scales")
install.packages("gridExtra")
require(grid)
install.packages("gbm")
install.packages("caret")
install.packages("tidyverse")
install.packages("data.table")
install.packages("doMC")
install.packages("e1071")
install.packages("corrplot")
install.packages("tidyr")
library(Amelia)
library(mice)
library(VIM)
library(pscl)
library(gplots)
library(pROC)
library(ROCR)
library(FactoMineR)
library(randomForest)
library(psych)
library(ggplot2)
library(scales)
library(gridExtra)
require(grid)
library(gbm)
library(caret)
library(tidyverse)
library(data.table)
library(doMC)
library(e1071)
library(corrplot)
library(tidyr)
```

#**Part I. Initial Exploration**
##**1.1 Missing values overview**
```{r echo=FALSE}
setwd("/Users/Joel/Library/Mobile Documents/com~apple~CloudDocs/Études (ESSEC)/MSc - Big Data Analytics/Case")
bankdata<-read.csv("bank-additional-full.csv", sep=";", na.strings=c("unknown","nonexistent"))

sapply(bankdata,function(x) sum(is.na(x))) #number of missing values
pMiss <- function(x){sum(is.na(x))/length(x)*100}
missing_per_poutcome<-pMiss(bankdata$poutcome)
missing_per_default<-pMiss(bankdata$default)
missmap(bankdata, main = "Missing values vs observed")
aggr_plot <- aggr(bankdata, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(bankdata), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```
Variables with lots of missing data points would be expected to end up with large error terms. Hence, we investigate the amount of missing values both numerically and graphically. Frome the above table and diagram, we see that over 86.34% of _poutcome_ and 20.8% of _default_,together with some other five variables, are missing. 

##**1.2 Data cleaning**
In the original dataset, _pdays_ takes "999" if the client was not previously contacted, which can not be interpreted in a numerical sense. Therefore, We first make some adjustments to _pdays_ so that all the values have practical meaning. Then we impute missing values on all variables except _poutcome_ and _duration_. 
```{r echo=FALSE} 
bankdata$pdays<-1/(bankdata$pdays+1)
bankdata$pdays[bankdata$pdays == 1/1000] <- 0

bankdata_model1 <-bankdata[,-c(11,15)] 

tempData1 <- mice(bankdata_model1,m=1,maxit=50,meth='pmm',seed=500)
```
Usually a safe threshold of missing values is 5% of the total for large datasets. Since the missing value percentage of _poutcome_ far exceed this number, we decide to do the listwise deletion and drop this feature from analysis for the time being. Also, according to the data specification, _duration_ should be discarded if our intention is to have predictive model. Therefore, we remove column 11 and 15 from our dataset, representing _duration_ and _poutcome_ respectively. We then apply MICE to the rest of data with missing values assuming that these values are missing at random (MAR). 

#**Part II.Data analysis with logit model - benchmark model**
##**2.1 Linear algorithms: Logistic regression model(GLM)**
The model we obtained below will be the benchmark model since we have done nothing to the dataset except imputing the missing values and transforming _pdays_. We apply the imputed dataset to the regression model by splitting the data into training set and test set in order to better evaluate the model. Since the response variable is binary, we therefore adopt the logit model. Then we intend to briefly evaluate the fitting of the model. we apply _anova_ to see how our model is doing against the null model (a model with only the intercept) and also obtain the McFadden R2 index to assess the model fit.
```{r echo=FALSE}
completedData1<-complete(tempData1,1)
train_sub1=sample(nrow(completedData1), 3/4*nrow(completedData1))
train_data1= completedData1[train_sub1,]
test_data1=completedData1[-train_sub1,]
dim(train_data1)
dim(test_data1)

benchmark_model <- glm(y ~.,family=binomial(link='logit'),data=train_data1)
summary(benchmark_model)

anova(benchmark_model, test = "Chisq") #table of deviance: how our model is doing against the null model (a model with only the intercept). The wider this gap, the better. 

pR2(benchmark_model) #assess the model fit
```
We find that _age_, _education_, _default_, _housing_, _euribor3m_ and _loan_ are not significant at all levels. Some interesting facts are that _bluecollar_ is more significant than any other job categories, and _day of week5 (ie. Friday)_ seems to help predict the result more than other week days. 

The deviances drop when adding each variable one at a time, but _job_, _contact_, _month_, _pdays_, _emp.var.rate_ and _cons.price.idx_ reduce the residual deviances more significantly than the others.

##**2.2 Prediction**
Then we proceed to evaluate how the model is doing when predicting y on a new set of data through applying the remaining 25% of data. We first set our desicion boundary at 0.5, meaning that if P(y=1|X) > 0.5 then y = "yes" otherwise y= "no". 
```{r echo=FALSE, message=FALSE, warning=FALSE}
fitted.results.benchmark <- predict(benchmark_model,newdata=test_data1,type='response')
fitted.results.benchmark <- ifelse(fitted.results.benchmark > 0.5,"yes","no") 

misClasificError <- mean(fitted.results.benchmark != test_data1$y)
print(paste('Accuracy',1-misClasificError))

fitted_value_bm<- table(fitted.results.benchmark)
real_value_bm<- table(test_data1$y)
comparison<-data.frame(cbind(real_value_bm,fitted_value_bm))
comparison
```
We calculate the accuracy rate by comparing the fitted result with its real value. The result is 0.9, which seems quite good. Nevertheless, in our case, over 86% of the response variable is "no". Therefore, the model will achieve a high accuracy rate even if it wrongly predict that all clients will not subscribe. In fact, the comparison table shows that the model did poorly when it came to "yes". 

Therefore, we plot the ROC curve and calculate the AUC which are typical measurements for a binary classifier.
```{r echo=FALSE, message=FALSE, warning=FALSE}
bm_predict <- predict(benchmark_model,newdata=test_data1,type='response')
pr<-prediction(bm_predict,test_data1$y)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
print(paste('auc',auc))
```

In the following steps, we are going to reduce dimensions of the dataset we used above and compare the prediction results among different models.

##**2.3 Linear algorithms: Logistic regression model(GLM) - balanced benchmark model**
```{r echo=FALSE}
#Create balanced datasets (after PCA, before running logistic regression and boosted trees)
##Create a dataset with 50% y=="yes" and 50% y=="no"
data.no<-bankdata_model1[bankdata_model1$y=="no",]
set.seed(3546)
data.50sample<-rbind(
  data.no[sample(nrow(data.no), 4640), ],
  bankdata_model1[bankdata_model1$y=="yes",])
rm(data.no)
##Divide into training and test datasets.
set.seed(3546)
index<-createDataPartition(
  data.50sample$y,
  p = 0.8,
  list = FALSE,
  times = 1)
data.train<-data.50sample[index,]
data.test<-data.50sample[-index,]
rm(index)

benchmark_model2 <- glm(y ~.,family=binomial(link='logit'),data=data.train)
summary(benchmark_model2)

fitted.results.benchmark2 <- predict(benchmark_model2,newdata=data.train,type='response')
fitted.results.benchmark2 <- ifelse(fitted.results.benchmark2 > 0.5,"yes","no") 

misClasificError <- mean(fitted.results.benchmark2 != data.train$y)
print(paste('Accuracy',1-misClasificError))

fitted_value_bm2<- table(fitted.results.benchmark2)
real_value_bm2<- table(data.train$y)
comparison<-data.frame(cbind(real_value_bm2,fitted_value_bm2))
comparison

bm_predict2 <- predict(benchmark_model2,newdata=data.test,type='response')
pr2<-prediction(bm_predict2,data.test$y)
prf2 <- performance(pr2, measure = "tpr", x.measure = "fpr")
plot(prf2)

auc2 <- performance(pr2, measure = "auc")
auc2 <- auc2@y.values[[1]]
print(paste('auc',auc2))
```

#**Part III.Further Exploration**
##**3.1. Variables Selection**
```{r echo=FALSE}
#Descriptive statistics
summary(bankdata_model1)

#Generate plots for numerical variables:
##gnumplots function (automatically generates plots for multiple numerical variables)
gnumplots<-function(data,varlist){
  plots.mtx<-matrix(list(),nrow=length(varlist),ncol=3)
  for(i in 1:length(varlist)){
    plots.mtx[[i,1]]<-ggplot(data,aes(x=get(varlist[[i]])))+
      geom_histogram(bins = 30)+
      xlab(varlist[[i]])
    plots.mtx[[i,2]]<-ggplot(data,aes(x=get(varlist[[i]]),fill=y,colour=y))+
      geom_density(alpha=0.25)+
      xlab(varlist[[i]])+
      scale_y_continuous(labels = percent)
    quant.values<-quantile(x = data[,varlist[[i]]], probs = seq(0,1,0.1))
    plotsrc<-data.frame(
      quant=c(1:10,1:10),
      y=rep(c("yes","no"),each=10),
      count=vector(mode = "integer",length = 20),
      freq=vector(mode = "double",length = 20))
    for (j in 1:20) {
      if (j %in% c(1,11)) {
        plotsrc$count[j]<-sum(data$y==plotsrc$y[j] & data[,varlist[[i]]]<=quant.values[plotsrc$quant[j]+1])
      } else {
        plotsrc$count[j]<-sum(data$y==plotsrc$y[j] & data[,varlist[[i]]]<=quant.values[plotsrc$quant[j]+1])-
          sum(data$y==plotsrc$y[j] & data[,varlist[[i]]]<=quant.values[plotsrc$quant[j-1]+1])
      }
    }
    for (j in 1:20) {
      if (sum(plotsrc$count[plotsrc$quant==plotsrc$quant[j]])>0) {
        plotsrc$freq[j]<-plotsrc$count[j]/sum(plotsrc$count[plotsrc$quant==plotsrc$quant[j]])
      } else {
        plotsrc$freq[j]<-plotsrc$freq[j-1]
      }
    }
    plots.mtx[[i,3]]<-ggplot(data=plotsrc,aes(x=quant,y=freq,fill=y))+
      geom_bar(stat="identity")+
      xlab(paste(varlist[[i]],"(quantiles)",sep = " "))+
      ylab("frequency")+
      scale_x_continuous(breaks = seq(1,10,1),labels=quant.values[2:11])+
      scale_y_continuous(labels=percent)+
      geom_label(aes(label=percent(freq)),
                 size=3,
                 position = position_stack(vjust = 0.5),
                 show.legend = FALSE)+
      guides(fill=guide_legend(title="y"))
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(2, 2)))
    print(plots.mtx[[i,1]],
          vp=viewport(layout.pos.row = 1, layout.pos.col = 1)
    )
    print(plots.mtx[[i,2]],
          vp=viewport(layout.pos.row = 1, layout.pos.col = 2)
    )
    print(plots.mtx[[i,3]],
          vp=viewport(layout.pos.row = 2, layout.pos.col = 1:2)
    )
  }
}
##Create the variable list and use gnumplots function 
numvarlist<-list(
  "age",
  "campaign",
  "pdays",
  "previous",
  "emp.var.rate",
  "cons.price.idx",
  "cons.conf.idx",
  "euribor3m",
  "nr.employed")
gnumplots(bankdata_model1,numvarlist)

#Generate plots for categorical variables
##gcatplots function (automatically generates plots for multiple categorical variables)
gcatplots<-function(data,varlist){
  plots.mtx<-matrix(list(),nrow=length(varlist),ncol=3)
  for (i in 1:length(varlist)){
    plots.mtx[[i,1]]<-ggplot(data,aes(x = get(varlist[[i]])))+
      geom_bar()+
      xlab(varlist[[i]])
    cont<-as.data.frame(table(data[,varlist[[i]]],data$y))
    freqy<-cont[cont$Var2=="yes",3]/sum(cont[cont$Var2=="yes",3])
    freqn<-cont[cont$Var2=="no",3]/sum(cont[cont$Var2=="no",3])
    dir<-names(table(data[,varlist[[i]]]))
    dfy<-data.frame(dir=dir,y=rep("yes",length(dir)),freq=freqy)
    dfn<-data.frame(dir=dir,y=rep("no",length(dir)),freq=freqn)
    df<-rbind(dfy,dfn)
    plots.mtx[[i,2]]<-ggplot(data=df,aes(x=y,y=freq,fill=dir))+
      geom_bar(stat="identity")+
      ylab("frequency")+
      scale_y_continuous(labels = percent)+
      guides(fill=guide_legend(title=varlist[[i]]))
    len<-nrow(cont)
    Freqk<-vector(mode="integer",length=len)
    for (k in 1:(len/2)){
      Freqk[k]<-cont[k,3]/(cont[k,3]+cont[k+len/2,3])
      Freqk[k+len/2]<-cont[k+len/2,3]/(cont[k,3]+cont[k+len/2,3])
    }
    cont$Freq<-Freqk
    plots.mtx[[i,3]]<-ggplot(data=cont,aes(x=Var1,y=Freq,fill=Var2))+
      geom_bar(stat="identity")+
      xlab(varlist[[i]])+
      ylab("frequency")+
      scale_y_continuous(labels = percent)+
      geom_label(aes(label=percent(Freq)),
                 size=3,
                 position = position_stack(vjust = 0.5),
                 show.legend = FALSE)+
      guides(fill=guide_legend(title="y"))
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(2, 2)))
    print(plots.mtx[[i,1]],
          vp=viewport(layout.pos.row = 1, layout.pos.col = 1)
    )
    print(plots.mtx[[i,2]],
          vp=viewport(layout.pos.row = 1, layout.pos.col = 2)
    )
    print(plots.mtx[[i,3]],
          vp=viewport(layout.pos.row = 2, layout.pos.col = 1:2)
    )
  }
}
##Create the variable list and use gcatplots function 
catvarlist<-list(
  "marital",
  "job",
  "education",
  "default",
  "housing",
  "loan",
  "contact",
  "month",
  "day_of_week")
gcatplots(bankdata_model1,catvarlist)
```
Remove variables (before PCA)
After looking at the plots, we removed all the variables which had little differences between y=="yes" and y=="no" respondents.
2 exceptions: campaign, education, as they make business sense (e.g. the more educated you are, the more likeky you know about deposits, the more likely you are going to subscribe).
Duration is also removed as it cannot be used to predict beforehand whether the consumer will subscribe.
pdays is removed as pdaysinv has been computed instead.

* _age_ doesn't seem to explain much the outcome, but we found that extremely older and extremely younger people tend to subscribe more to the deposit.
* We should delete _campaign_ as there seems to be no direct correlation with y.
* We observe that people who have never been contacted before are much less likely to subscribe than people who have, so we will transform _pdays_ as a binary variable.
* _previous_ stays unchanged.
* _emp.var.rate_ stays unchanged.
* _cons.price.idx_ stays unchanged.
* _cons.conf.idx_ stays unchanged.
* _euribor3m_ stays changed.
* _nr.employed_ stays changed.
* We should delete _marital_ as there seems to be no direct correlation with y.
* _job_ stays changed.
* _education_ stays changed.
* We should delete _default_ as there seems to be no direct correlation with y.
* We should delete _housing_ as there seems to be no direct correlation with y.
* We should delete _loan_ as there seems to be no direct correlation with y.
* _contact_ stays changed.
* _month_ stays changed.
* We should delete _day of week_ as there seems to be no direct correlation with y.

```{r echo=FALSE}
rm.var<-c(
  "campaign",
  "marital",
  "default",
  "housing",
  "loan",
  "day_of_week"
)
bankdata_model1<-bankdata_model1[,!names(bankdata_model1) %in% rm.var]

bankdata_model2<-bankdata_model1
bankdata_model2$pdays[bankdata_model2$pdays>0,]<-1
bankdata_model2$pdays[bankdata_model2$pdays==0,]<-0


#Create balanced datasets (after PCA, before running logistic regression and boosted trees)
##Create a dataset with 50% y=="yes" and 50% y=="no"
data.no<-data[data$y=="no",]
set.seed(3546)
data.50sample<-rbind(
  data.no[sample(nrow(data.no), 4640), ],
  data[data$y=="yes",])
rm(data.no)
##Divide into training and test datasets.
set.seed(3546)
index<-createDataPartition(
  data.50sample$y,
  p = 0.8,
  list = FALSE,
  times = 1)
data.train<-data.50sample[index,]
data.test<-data.50sample[-index,]
rm(index)
```

PCA


##**3.2. Methodology**
###**3.2.1. Bagging algorithms: Random Forest**
```{r echo=FALSE}
train<-train_data1
test<-test_data1
model_rf <- randomForest(y ~ ., data = train, importance=TRUE)

varImpPlot(model_rf)

pr_rf<-predict(model_rf,newdata=test)
table(pr_rf, test$y)

accuray<-(8905+352)/nrow(test)
print(paste('Accuracy',accuray))

predictions=as.vector(rf_output$votes[,2])
pred=prediction(predictions,target)

perf_AUC=performance(pred,"auc") #Calculate the AUC value
AUC=perf_AUC@y.values[[1]]

perf_ROC=performance(pred,"tpr","fpr") #plot the actual ROC curve
plot(perf_ROC, main="ROC plot")
text(0.5,0.5,paste("AUC = ",format(AUC, digits=5, scientific=FALSE)))

modelroc_rf=roc(test_data1$y,as.numeric(pr_rf))
plot(modelroc_rf,print.auc=TRUE,auc.polygon=TRUE,grid=c(0.1,0.2),grid.col=c("green","red"),max.auc.polygon=TRUE,auc.polygon.col="green",print.thres=TRUE)
```
The accuracy one tests to see how worse the model performs without each variable, so a high decrease in accuracy would be expected for very predictive variables. The Gini tests to see the result if each variable is taken out and a high score means the variable was important.

###**3.2.2. Boosting algorithms: Stochastic Gradient Boosting (GBM)**
```{r echo=FALSE}
#Convert y to numeric binary values
datab<-data
levels(datab$y)[levels(datab$y)=="yes"]<-1
levels(datab$y)[levels(datab$y)=="no"]<-0
datab$y<-as.numeric(datab$y)-1

#Run boosted trees model
bst<-gbm(formula = y~.,
         data = datab,
         distribution = "bernoulli",
         n.trees = 200,
         cv.folds = 4,
         interaction.depth = 2,
         n.minobsinnode = 10,
         shrinkage = 0.05,
         bag.fraction = 0.5,
         train.fraction = 0.5,
         keep.data = FALSE,
         verbose = TRUE,
         n.cores = 1)
pred<-predict(bst,data,type = "response")

#Draw ROC curve
ROCC<-roc(datab$y,pred)
plot(ROCC,print.thres="best")

#Generate confusion matrix
pred.binary<-as.numeric(pred>coords(ROCC,x="best",ret="threshold"))
table(datab$y,pred.binary)
#or (better, require caret package)
confusionMatrix(datab$y,pred.binary,positive = "1")
```


Data source:
  [Moro et al., 2011] S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. 
  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011. EUROSIS."
  Available at: [pdf] http://hdl.handle.net/1822/14838  
                [bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt
